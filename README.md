# Reinforcement Learning with Robust Stability Guarantee

## Conda environment
From the general python package sanity perspective, it is a good idea to use conda environments to make sure packages from different projects do not interfere with each other.


To create a conda env with python3, one runs 
```bash
conda create -n test python=3.6
```
To activate the env: 
```
conda activate test
```

# Installation Environment

```bash
git clone https://github.com/RobustStabilityGuaranteeRL/RobustStabilityGuaranteeRL
pip install numpy==1.16.3
pip install tensorflow==1.13.1
pip install tensorflow-probability==0.6.0
pip install opencv-python
pip install cloudpickle
pip install gym
pip install matplotlib

```


### Example 1. RLAC with continous cartpole
```
python main.py
```
The hyperparameters, the tasks and the learning algorithm can be changed via change the variant.py, for example:


The algorithm_name could be one of ['RLAC','RARL','SAC_cost']


Other hyperparameter are also ajustable in variant.py.
```bash
VARIANT = {
    'env_name': 'cartpole_cost',
    #training prams
    'algorithm_name': 'RLAC',
    # 'algorithm_name': 'RARL',
    # 'algorithm_name': 'SAC_cost',
    'disturber': 'SAC',
    'additional_description': '',
    # 'evaluate': False,
    'train': True,
    # 'train': False,
    'num_of_trials': 10,   # number of random seeds
    'store_last_n_paths': 10,  # number of trajectories for evaluation during training
    'start_of_trial': 0,
    #evaluation params
    'evaluation_form': 'impulse',
    # 'evaluation_form': 'param_variation',
    # 'evaluation_form': 'trained_disturber',
    'eval_list': [
        # 'RLAC',
    ],
    'trials_for_eval': [str(i) for i in range(0, 10)],
    'evaluation_frequency': 2048,
}
```
### Figures
<div align=center><img src = "https://github.com/RobustStabilityGuaranteeRL/RobustStabilityGuaranteeRL/blob/master/figures/cartpole/training-return-1.jpg" width=400 alt="figure"></div>
<div align=center>Figure 1. Cumulative cost curves for RLAC, RARL and SAC in the Cartpole environment; the shadowed region shows the 1-SD confidence interval over 10 random seeds and the X-axis indicates the total time steps in thousand.</div>

<div align=center><img src = "https://github.com/RobustStabilityGuaranteeRL/RobustStabilityGuaranteeRL/blob/master/figures/cartpole/impulse/impulse-death_rate-2.jpg" width=400 alt="figure"></div>
<div align=center>Figure 2. Death rate of agents trained by RLAC, RARL, SAC and LQR in the presence of instant impulsive force $F$ with different magnitudes in the Cartpole environment. The trained policies are initialized by 10 random seeds. The policies with different initializations are evaluated equally for 500 episodes. The line indicates the average death rate of these policies and shadowed region shows the 1-SD confidence interval.</div>

<div align=center><img src = "https://github.com/RobustStabilityGuaranteeRL/RobustStabilityGuaranteeRL/blob/master/figures/cartpole/constant_impulse-death_rate-3.jpg" width=400 alt="figure"></div>
<div align=center>Figure 3. Death rate of agents trained by RLAC, RARL, SAC and LQR in the presence of persistent impulsive force $F$ with different magnitudes. The impulses are applied on the cartpole every 20 steps, each algorithm are evaluated for 100 times under different magnitudes. </div>

<div align=center><img src = "https://github.com/RobustStabilityGuaranteeRL/RobustStabilityGuaranteeRL/blob/master/figures/cartpole/various_disturbance-sin-death_rate-4.jpg" width=400 alt="figure"></div>
<div align=center>Figure 4. Death rate of agents trained by RLAC, RARL, SAC and LQR in the presence of disturbance force $F$ generated by sine waves with different frequencies in the Cartpole environment. The initial phase positions of the sine function are randomized while the maximum magnitude is fixed at 80.</div>

<div align=center><img src = "https://github.com/RobustStabilityGuaranteeRL/RobustStabilityGuaranteeRL/blob/master/figures/cartpole/param_variation/@@figure2-5.jpg" width=400 alt="figure"></div>
<div align=center>Figure 5. Death rate and total costs of agents trained by RLAC, RARL, SAC and LQR in the presence of different parametric uncertainty which are \emph{unseen} during training and different from dynamic randomization in the Cartpole environment. $l$ (X-axis) and $m_c$ (Y-axis) vary with the step size of $0.1$ and $0.2$ respectively. At each point of the parameter grid, the results are averaged between the agents with different initializations over 100 episodes.</div>

<div align=center><img src = "https://github.com/RobustStabilityGuaranteeRL/RobustStabilityGuaranteeRL/blob/master/figures/halfcheetah/training-return-6.jpg" width=400 alt="figure"></div>
<div align=center>Figure 6. Cumulative cost curves for RLAC, RARL and SAC in the HalfCheetah environment; the shadowed region shows the 1-SD confidence interval over 5 random seeds and the X-axis indicates the total time steps in thousand.</div>

<div align=center><img src = "https://github.com/RobustStabilityGuaranteeRL/RobustStabilityGuaranteeRL/blob/master/figures/halfcheetah/constant_impulse-return-7.jpg" width=400 alt="figure"></div>
<div align=center>Figure 7. Cumulative cost of agents trained by RLAC, RARL, SAC in the presence of persistent impulsive force $F$ with different magnitudes. The impulses are applied on the HalfCheetah every 20 steps and each algorithm are evaluated for 100 times under different magnitudes.</div>

<div align=center><img src = "https://github.com/RobustStabilityGuaranteeRL/RobustStabilityGuaranteeRL/blob/master/figures/halfcheetah/various_disturbance-sin-return-8.jpg" width=400 alt="figure"></div>
<div align=center>Figure 8. Cumulative cost of agents trained by RLAC, RARL, SAC in the presence of disturbance force $F$ generated by sine waves with different frequencies in the HalfCheetah environment. The initial phase positions of the sine function are randomized while the maximum magnitude is fixed at 1.</div>

<div align=center><img src = "https://github.com/RobustStabilityGuaranteeRL/RobustStabilityGuaranteeRL/blob/master/figures/cartpole/impulse/impulse-death_rate_comparison-9.jpg" width=400 alt="figure"></div>
<div align=center>Figure 9. Death rate of agents trained by RLAC with Lyapunov function candidates of different length of horizon, in the presence of instant impulsive force $F$. </div>

### Video
<div align=center><img src = "https://github.com/RobustStabilityGuaranteeRL/RobustStabilityGuaranteeRL/blob/master/figures/video-gif.gif" width=600 alt="figure"></div>
<div align=center>Video</div>

Visulization of agents trained by RLAC and RARL in presence of instant and persistant impulses and sine wave disturbances in the Cartpole environment.
## Reference


[1] [Reinforcement-learning-with-tensorflow](https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow)

[2] [Gym](https://github.com/openai/gym)
